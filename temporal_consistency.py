"""
Syst√®me de coh√©rence temporelle pour g√©n√©ration vid√©o
√âvite les changements abrupts entre frames
Utilise ControlNet, img2img, et frame interpolation
"""

import torch
import numpy as np
from PIL import Image
import cv2
from typing import List, Optional, Tuple
from diffusers import (
    StableDiffusionControlNetPipeline,
    StableDiffusionImg2ImgPipeline,
    ControlNetModel,
    DDIMScheduler
)
from controlnet_aux import CannyDetector, OpenposeDetector, MidasDetector
import os


class TemporalConsistencyEngine:
    """Assure la coh√©rence temporelle entre les frames vid√©o"""

    def __init__(self, device: str = "cuda"):
        self.device = device
        self.controlnet_pipe = None
        self.img2img_pipe = None
        self.canny_detector = CannyDetector()
        self.depth_detector = None
        self.previous_frame = None
        self.previous_latent = None

    def load_models(self, model_name: str = "stabilityai/stable-diffusion-xl-base-1.0"):
        """Charge les mod√®les pour coh√©rence temporelle"""
        print("üîß Chargement des mod√®les de coh√©rence temporelle...")

        try:
            # ControlNet pour guidance structurelle
            controlnet = ControlNetModel.from_pretrained(
                "diffusers/controlnet-canny-sdxl-1.0",
                torch_dtype=torch.float16
            )

            # Pipeline ControlNet
            self.controlnet_pipe = StableDiffusionControlNetPipeline.from_pretrained(
                model_name,
                controlnet=controlnet,
                torch_dtype=torch.float16,
                safety_checker=None
            ).to(self.device)

            # Pipeline img2img pour coh√©rence frame-to-frame
            self.img2img_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                safety_checker=None
            ).to(self.device)

            # Utiliser DDIM scheduler pour meilleure coh√©rence
            self.controlnet_pipe.scheduler = DDIMScheduler.from_config(
                self.controlnet_pipe.scheduler.config
            )
            self.img2img_pipe.scheduler = DDIMScheduler.from_config(
                self.img2img_pipe.scheduler.config
            )

            # Activer attention slicing pour √©conomiser VRAM
            self.controlnet_pipe.enable_attention_slicing()
            self.img2img_pipe.enable_attention_slicing()

            print("‚úì Mod√®les charg√©s avec succ√®s")
            return True

        except Exception as e:
            print(f"‚ùå Erreur lors du chargement des mod√®les: {e}")
            return False

    def generate_first_frame(self,
                            prompt: str,
                            negative_prompt: str,
                            width: int,
                            height: int,
                            steps: int,
                            seed: int,
                            control_image: Optional[Image.Image] = None) -> Image.Image:
        """
        G√©n√®re la premi√®re frame avec ControlNet si une image de contr√¥le est fournie

        Args:
            prompt: Prompt de g√©n√©ration
            negative_prompt: Negative prompt
            width, height: Dimensions
            steps: Nombre de steps
            seed: Seed
            control_image: Image de contr√¥le (heightmap, canny, etc.)

        Returns:
            Premi√®re frame g√©n√©r√©e
        """
        generator = torch.Generator(device=self.device).manual_seed(seed)

        if control_image is not None and self.controlnet_pipe is not None:
            # G√©n√©rer avec ControlNet pour structure coh√©rente
            control_image = control_image.resize((width, height))

            # Extraire les contours Canny
            canny_image = self.canny_detector(control_image)

            image = self.controlnet_pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                image=canny_image,
                width=width,
                height=height,
                num_inference_steps=steps,
                generator=generator,
                controlnet_conditioning_scale=0.5
            ).images[0]
        else:
            # G√©n√©ration standard sans contr√¥le
            if self.img2img_pipe is not None:
                # Cr√©er une image initiale al√©atoire
                init_image = Image.new('RGB', (width, height), color=(128, 128, 128))
                image = self.img2img_pipe(
                    prompt=prompt,
                    negative_prompt=negative_prompt,
                    image=init_image,
                    strength=1.0,
                    num_inference_steps=steps,
                    generator=generator
                ).images[0]
            else:
                raise ValueError("Aucun pipeline disponible")

        # Sauvegarder pour la prochaine frame
        self.previous_frame = image

        return image

    def generate_next_frame(self,
                           prompt: str,
                           negative_prompt: str,
                           steps: int,
                           seed: int,
                           strength: float = 0.3,
                           control_image: Optional[Image.Image] = None) -> Image.Image:
        """
        G√©n√®re la frame suivante en utilisant la frame pr√©c√©dente comme base

        Args:
            prompt: Prompt (peut varier l√©g√®rement)
            negative_prompt: Negative prompt
            steps: Nombre de steps
            seed: Seed
            strength: Force de transformation (0.1-0.5 pour coh√©rence)
            control_image: Image de contr√¥le (heightmap de la m√™me montagne)

        Returns:
            Frame suivante coh√©rente
        """
        if self.previous_frame is None:
            raise ValueError("Appelez d'abord generate_first_frame()")

        generator = torch.Generator(device=self.device).manual_seed(seed)

        if control_image is not None and self.controlnet_pipe is not None:
            # Utiliser ControlNet + img2img pour coh√©rence maximale
            width, height = self.previous_frame.size
            control_image = control_image.resize((width, height))
            canny_image = self.canny_detector(control_image)

            # G√©n√©rer avec guidance structurelle ET frame pr√©c√©dente
            image = self.controlnet_pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                image=canny_image,
                num_inference_steps=steps,
                generator=generator,
                controlnet_conditioning_scale=0.3  # Plus faible pour variation
            ).images[0]

            # Blend avec frame pr√©c√©dente
            blend_factor = 1.0 - strength
            image = Image.blend(self.previous_frame, image, strength)

        else:
            # Img2img pur avec frame pr√©c√©dente
            image = self.img2img_pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                image=self.previous_frame,
                strength=strength,  # Faible strength = haute coh√©rence
                num_inference_steps=steps,
                generator=generator
            ).images[0]

        # Sauvegarder pour la prochaine frame
        self.previous_frame = image

        return image

    def reset(self):
        """Reset l'√©tat pour une nouvelle s√©quence"""
        self.previous_frame = None
        self.previous_latent = None


class FrameInterpolator:
    """Interpolation entre frames pour fluidit√©"""

    def __init__(self):
        self.method = "optical_flow"

    def interpolate_optical_flow(self,
                                 frame1: Image.Image,
                                 frame2: Image.Image,
                                 num_intermediate: int = 3) -> List[Image.Image]:
        """
        Interpole entre deux frames avec optical flow

        Args:
            frame1: Premi√®re frame
            frame2: Deuxi√®me frame
            num_intermediate: Nombre de frames interm√©diaires

        Returns:
            Liste de frames interpol√©es
        """
        # Convertir en numpy
        img1 = np.array(frame1)
        img2 = np.array(frame2)

        # Convertir en grayscale pour optical flow
        gray1 = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY)
        gray2 = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY)

        # Calculer optical flow
        flow = cv2.calcOpticalFlowFarneback(
            gray1, gray2,
            None,
            pyr_scale=0.5,
            levels=3,
            winsize=15,
            iterations=3,
            poly_n=5,
            poly_sigma=1.2,
            flags=0
        )

        # G√©n√©rer les frames interm√©diaires
        interpolated = []
        for i in range(1, num_intermediate + 1):
            t = i / (num_intermediate + 1)

            # Warping de l'image 1 vers l'image 2
            h, w = flow.shape[:2]
            flow_t = flow * t
            map_x = np.tile(np.arange(w), (h, 1)).astype(np.float32)
            map_y = np.tile(np.arange(h), (w, 1)).T.astype(np.float32)
            map_x += flow_t[:, :, 0]
            map_y += flow_t[:, :, 1]

            warped1 = cv2.remap(img1, map_x, map_y, cv2.INTER_LINEAR)

            # Blend simple
            alpha = t
            blended = cv2.addWeighted(warped1, 1-alpha, img2, alpha, 0)

            interpolated.append(Image.fromarray(blended))

        return interpolated

    def interpolate_simple_blend(self,
                                 frame1: Image.Image,
                                 frame2: Image.Image,
                                 num_intermediate: int = 3) -> List[Image.Image]:
        """
        Interpolation simple par blending

        Args:
            frame1: Premi√®re frame
            frame2: Deuxi√®me frame
            num_intermediate: Nombre de frames interm√©diaires

        Returns:
            Liste de frames interpol√©es
        """
        interpolated = []
        for i in range(1, num_intermediate + 1):
            alpha = i / (num_intermediate + 1)
            blended = Image.blend(frame1, frame2, alpha)
            interpolated.append(blended)

        return interpolated


class VideoCoherenceManager:
    """Gestionnaire principal pour vid√©os coh√©rentes"""

    def __init__(self, device: str = "cuda"):
        self.temporal_engine = TemporalConsistencyEngine(device)
        self.interpolator = FrameInterpolator()
        self.heightmap_sequence = []

    def load_models(self, model_name: str = "stabilityai/stable-diffusion-xl-base-1.0"):
        """Charge les mod√®les n√©cessaires"""
        return self.temporal_engine.load_models(model_name)

    def generate_coherent_video(self,
                               base_prompt: str,
                               negative_prompt: str,
                               camera_params: List[dict],
                               heightmap_base: Optional[np.ndarray],
                               width: int = 1024,
                               height: int = 768,
                               steps: int = 30,
                               seed: int = 42,
                               strength: float = 0.25,
                               interpolate: bool = True,
                               interpolation_frames: int = 2) -> List[Image.Image]:
        """
        G√©n√®re une vid√©o coh√©rente avec la m√™me montagne vue sous diff√©rents angles

        Args:
            base_prompt: Prompt de base
            negative_prompt: Negative prompt
            camera_params: Liste de param√®tres de cam√©ra pour chaque frame
            heightmap_base: Heightmap de la montagne (reste identique)
            width, height: Dimensions
            steps: Steps de diffusion
            seed: Seed de base
            strength: Force de variation entre frames (0.1-0.4 recommand√©)
            interpolate: Ajouter des frames interpol√©es
            interpolation_frames: Nombre de frames interpol√©es entre chaque frame g√©n√©r√©e

        Returns:
            Liste de toutes les frames
        """
        all_frames = []

        # Convertir heightmap en image de contr√¥le
        control_image = None
        if heightmap_base is not None:
            control_image = Image.fromarray(
                (heightmap_base * 255).astype(np.uint8),
                mode='L'
            ).convert('RGB')

        # G√©n√©rer la premi√®re frame
        print(f"üé¨ G√©n√©ration frame 1/{len(camera_params)}")
        first_frame = self.temporal_engine.generate_first_frame(
            prompt=base_prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            steps=steps,
            seed=seed,
            control_image=control_image
        )
        all_frames.append(first_frame)

        # G√©n√©rer les frames suivantes avec coh√©rence
        for i, cam_params in enumerate(camera_params[1:], start=2):
            print(f"üé¨ G√©n√©ration frame {i}/{len(camera_params)}")

            # Variation subtile du prompt selon la cam√©ra
            # mais SANS changer la montagne elle-m√™me
            varied_prompt = base_prompt  # On garde le m√™me prompt !

            # G√©n√©rer frame coh√©rente
            frame = self.temporal_engine.generate_next_frame(
                prompt=varied_prompt,
                negative_prompt=negative_prompt,
                steps=steps,
                seed=seed + i,  # L√©g√®re variation de seed
                strength=strength,  # Faible strength = haute coh√©rence
                control_image=control_image  # M√™me heightmap !
            )

            # Interpoler avec la frame pr√©c√©dente si demand√©
            if interpolate and len(all_frames) > 0:
                print(f"  ‚Ü≥ Interpolation de {interpolation_frames} frames...")
                interpolated = self.interpolator.interpolate_optical_flow(
                    all_frames[-1], frame, interpolation_frames
                )
                all_frames.extend(interpolated)

            all_frames.append(frame)

        # Reset pour la prochaine vid√©o
        self.temporal_engine.reset()

        return all_frames

    def generate_from_heightmap_rotation(self,
                                        heightmap: np.ndarray,
                                        base_prompt: str,
                                        negative_prompt: str,
                                        num_frames: int = 24,
                                        width: int = 1024,
                                        height: int = 768,
                                        steps: int = 30,
                                        seed: int = 42) -> List[Image.Image]:
        """
        G√©n√®re une rotation autour de la m√™me montagne (heightmap)

        La heightmap reste identique, seul l'angle de vue change

        Args:
            heightmap: Heightmap 3D de la montagne
            base_prompt: Prompt de base
            negative_prompt: Negative prompt
            num_frames: Nombre de frames pour rotation compl√®te
            width, height: Dimensions
            steps: Steps
            seed: Seed

        Returns:
            Frames de rotation
        """
        # TODO: Impl√©menter la rotation r√©elle du heightmap en 3D
        # Pour l'instant, on utilise le syst√®me de coh√©rence avec m√™me heightmap

        camera_params = [{'angle': i * 360 / num_frames} for i in range(num_frames)]

        return self.generate_coherent_video(
            base_prompt=base_prompt,
            negative_prompt=negative_prompt,
            camera_params=camera_params,
            heightmap_base=heightmap,
            width=width,
            height=height,
            steps=steps,
            seed=seed,
            strength=0.2,  # Tr√®s faible pour coh√©rence maximale
            interpolate=True,
            interpolation_frames=1
        )
